{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044d24d-f5e8-463e-8ee3-e6697611014b",
   "metadata": {},
   "source": [
    "### Human Activity Recognition (HAR):\n",
    "- __Definition:__ Identifying specific movements or actions of a person using sensor data.\n",
    "- __Typical Activities:__ Includes walking, talking, standing, sitting, and more focused activities like cooking or factory work.\n",
    "- __Sensor Data Sources:__\n",
    "  - Remote Recording: Video, radar, or other wireless methods.\n",
    "  - Direct Recording: Sensors on the person, such as accelerometers and gyroscopes in smartphones or custom hardware.\n",
    "- __Historical Context:__\n",
    "  1. Challenges:\n",
    "      - Sensor data collection was once challenging and expensive, requiring custom hardware.\n",
    "  3. Modern Solutions:\n",
    "      - Smartphones and Personal Devices: Now ubiquitous and inexpensive, making sensor data collection easier and more common.\n",
    "      - Fitness and Health Monitoring: Common applications of HAR with readily available data.\n",
    "- __Problem Statement:__\n",
    "  - Objective: Predict the activity from a snapshot of sensor data.\n",
    "  - Data Types: Typically involves univariate or multivariate time series data from one or more sensor types.\n",
    "- __Challenges:__\n",
    "  - Data Variability: Different subjects perform activities differently, leading to variations in sensor data.\n",
    "  - Modeling Difficulty: No direct way to relate sensor data to specific activities.\n",
    "- __Approach:__\n",
    "  - Data Collection: Record sensor data and corresponding activities from specific subjects.\n",
    "  - Model Training: Fit a model using this data.\n",
    "  - Generalization: Use the trained model to classify activities of new, unseen subjects based on their sensor data.\n",
    "\n",
    "Two main neural network approaches are effective for time series classification and have shown strong performance in activity recognition using sensor data from smartphones and fitness trackers:\n",
    "1. Convolutional Neural Network (CNN) Models.\n",
    "2. Recurrent Neural Network (RNN) Models.\n",
    "   \n",
    "__Recommendations:__\n",
    "- __RNN and LSTM:__\n",
    "  - Best For: Recognizing short activities with a natural order.\n",
    "  - Reason: They utilize the time-order relationship between sensor readings.\n",
    "- __CNN:__\n",
    "  - Best For: Inferring long-term repetitive activities.\n",
    "  - Reason: They excel at learning deep features in recursive patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e348808-cea7-4f13-9610-8feccca7ac68",
   "metadata": {},
   "source": [
    "### Data Preparation for Time Series Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc98ab-b46b-4393-ab1c-acab453df70c",
   "metadata": {},
   "source": [
    "__Sliding Window Approach:__\n",
    "- Definition: Dividing input signal data into windows of signals, where each window may contain one to a few seconds of observation data. This is often called a sliding window.\n",
    "- Usage: Applied in both classical machine learning methods on hand-crafted features and neural networks.\n",
    "  \n",
    "__Window Size Considerations:__ \n",
    "No Best Window Size: Depends on the model, nature of the sensor data, and the activities being classified.\n",
    "\n",
    "__Trade-offs:__\n",
    "- Larger Windows: Require larger models, slower to train.\n",
    "- Smaller Windows: Require smaller models, faster to train and fit.\n",
    "  \n",
    "__Intuitive Effects:__\n",
    "- Smaller Windows: Faster activity detection, reduced resource and energy needs.\n",
    "- Larger Windows: Better for recognizing complex activities.\n",
    "  \n",
    "__Overlapping Windows:__\n",
    "- Purpose: Mitigates the risk of missing transitions between activities by overlapping the end of one window with the start of the next.\n",
    "- Common Overlap: 50%, where the first half of the new window contains the last half of the previous window.\n",
    "  \n",
    "__Risks:__\n",
    "- Transition Errors: Errors can appear at the beginning or end of activities.\n",
    "- Incorrect Lengths: Truncated activity instances due to improper window lengths.\n",
    "  \n",
    "__Overlap in Neural Networks:__\n",
    "- Effect:\n",
    "  - Increases Training Data: A 50% overlap doubles the size of the training data, useful for smaller datasets.\n",
    "  - Risk of Overfitting: Larger training data can lead to overfitting.\n",
    "- Usage:\n",
    "  - Common in Some Applications: Overlapping windows are tolerated and useful in specific contexts.\n",
    "  - Less Frequent: Not always necessary and less frequently used with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113ae95-1576-4d7c-b4f8-38410c343d87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Combined CNN-LSTM Model for HAR:\n",
    "- __Common Approach:__ Using an LSTM in conjunction with a CNN for Human Activity Recognition (HAR) problems.\n",
    "- __Model Types:__\n",
    "  1. CNN-LSTM Model.\n",
    "  2. ConvLSTM Model.\n",
    "- __How It Works:__\n",
    "- CNN:\n",
    "  - Purpose: Extracts features from subsequences of raw sample data.\n",
    "  - Function: Processes short-term dependencies and patterns in the data.\n",
    "- LSTM:\n",
    "  - Purpose: Interprets the features extracted by the CNN.\n",
    "  - Function: Aggregates and models long-term dependencies and sequential relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372c8c0-14c0-4208-a833-5b48cf4faaa8",
   "metadata": {},
   "source": [
    "### Human Activity Recognition Dataset\n",
    "- __Problem:__ Classifying sequences of accelerometer data from specialized harnesses or smartphones into known movements.\n",
    "- __Challenges:__\n",
    "  - Large number of observations per second.\n",
    "  - Temporal nature of observations.\n",
    "  - Difficulty in relating accelerometer data to specific movements.\n",
    "- __Standard Dataset:__ Activity Recognition Using Smartphones\n",
    "  - Published: 2012 by Davide Anguita et al., University of Genova, Italy.\n",
    "  - Paper: \"A Public Domain Dataset for Human Activity Recognition Using Smartphones\" (2013).\n",
    "  - Link: https://www.esann.org/sites/default/files/proceedings/legacy/es2013-84.pdf.\n",
    "- __Dataset Details:__\n",
    "  - Participants: 30 volunteers aged 19-48 years.\n",
    "  - Protocol:\n",
    "       - Each participant performed activities wearing a waist-mounted Samsung Galaxy S II smartphone.\n",
    "       - Six activities: standing, sitting, lying down, walking, walking downstairs, walking upstairs.\n",
    "       - Each activity was performed twice: first with the phone fixed on the left side of the belt, then as preferred by the user.\n",
    "  - Pre-processing:\n",
    "       - Noise filtering on accelerometer and gyroscope data.\n",
    "       - Data split into fixed windows of 2.56 seconds (128 data points) with 50% overlap.\n",
    "       - Accelerometer data split into gravitational (total) and body motion components.    \n",
    "- __Dataset Usage:__\n",
    "  - Link: https://raw.githubusercontent.com/jbrownlee/Datasets/master/HAR_Smartphones.zip\n",
    "  - Download all the files in your working directory, unzip them, and rename the folder to \"HARDataset\".\n",
    "- __Contents:__\n",
    "  - Directories:\n",
    "       - train: Training data (70% of the dataset).\n",
    "       - test: Testing data (30% of the dataset).\n",
    "  - Files:\n",
    "       - README.txt: Detailed technical description.\n",
    "       - features.txt: Description of engineered features.     \n",
    "- __Train and Test Folders:__ Both folders contain similar files but with different data.\n",
    "- __Important Files in Train Folder:__\n",
    "  - Inertial Signals Folder: Contains preprocessed data.\n",
    "  - X_train.txt: Engineered features for model fitting.\n",
    "  - y_train.txt: Class labels for each observation (1-6).\n",
    "  - subject_train.txt: Mapping of each data record to a subject identifier (1-30).\n",
    "- __Inertial Signals Directory:__\n",
    "  - Gravitational Acceleration Data: total_acc_x_train.txt, total_acc_y_train.txt, total_acc_z_train.txt.\n",
    "  - Body Acceleration Data: body_acc_x_train.txt, body_acc_y_train.txt, body_acc_z_train.txt.\n",
    "  - Body Gyroscope Data: body_gyro_x_train.txt, body_gyro_y_train.txt, body_gyro_z_train.txt.\n",
    "- __Data Format:__\n",
    "  - Separation: Columns are separated by whitespace.\n",
    "  - Scaling: Values appear scaled to the range -1 to 1, confirmed by the README.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80623a6-8627-443a-8212-7e5f812d10d5",
   "metadata": {},
   "source": [
    "### Load and Explore Human Activity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67674e93-68d3-4aed-b216-79bdd70ba669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92863185-fe01-407e-ba96-3ba3faaf1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single file from the HAR dataset as a numpy array\n",
    "def load_file(filepath):\n",
    "    \"\"\"\n",
    "    Load a file from the HAR dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    filepath (str): The path to the file to load.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The loaded data.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffb86f1c-2747-4300-bdef-bf5bdfb7f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load the total_acc_y_train.txt file and print its shape\n",
    "data = load_file('HARDataset/train/Inertial Signals/total_acc_y_train.txt')\n",
    "# The training data is comprised of 7,352 rows or windows of data, where each window has 128 observations\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "624c0ac0-b065-43ef-be39-743720078d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1232167, -0.1268756, -0.1240037, -0.1249279, -0.1257667,\n",
       "       -0.124462 , -0.1273606, -0.1278912, -0.1258682, -0.1243682,\n",
       "       -0.1231382, -0.1213345, -0.1183578, -0.120062 , -0.1221186,\n",
       "       -0.12008  , -0.1209017, -0.1213949, -0.1215677, -0.1246812,\n",
       "       -0.1254896, -0.1249345, -0.1249063, -0.1249926, -0.1251552,\n",
       "       -0.1247985, -0.1254793, -0.1268068, -0.1272888, -0.123713 ,\n",
       "       -0.1192631, -0.1226967, -0.1271224, -0.126278 , -0.1261419,\n",
       "       -0.1251686, -0.121594 , -0.1190558, -0.1179128, -0.1174034,\n",
       "       -0.1172102, -0.1181487, -0.1185709, -0.1179084, -0.1205067,\n",
       "       -0.1243031, -0.1256299, -0.1246896, -0.1218014, -0.1202801,\n",
       "       -0.1206562, -0.1210648, -0.1216185, -0.1241114, -0.1280997,\n",
       "       -0.1280257, -0.126537 , -0.1274474, -0.1273523, -0.1264597,\n",
       "       -0.1247455, -0.1236691, -0.1229069, -0.1215528, -0.123976 ,\n",
       "       -0.1268078, -0.1277862, -0.1266547, -0.1236336, -0.1249187,\n",
       "       -0.1243005, -0.1197982, -0.1192223, -0.120174 , -0.1213162,\n",
       "       -0.1222915, -0.1264941, -0.132631 , -0.1286242, -0.122015 ,\n",
       "       -0.1218544, -0.1212108, -0.120603 , -0.1209479, -0.1212181,\n",
       "       -0.1236811, -0.1255531, -0.1241987, -0.1221575, -0.1233974,\n",
       "       -0.127263 , -0.1302381, -0.1312862, -0.1299998, -0.1277708,\n",
       "       -0.1239092, -0.1197379, -0.121468 , -0.1260118, -0.1282198,\n",
       "       -0.1263671, -0.1217475, -0.1218799, -0.1239658, -0.1242567,\n",
       "       -0.1261499, -0.1276499, -0.1295878, -0.1303476, -0.1299795,\n",
       "       -0.1321647, -0.12887  , -0.124246 , -0.1267462, -0.1270445,\n",
       "       -0.1256371, -0.12678  , -0.1258547, -0.1234932, -0.1219945,\n",
       "       -0.1239103, -0.1279703, -0.1282954, -0.1270103, -0.1261848,\n",
       "       -0.1240696, -0.1227451, -0.121326 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c030932-eaca-4cb8-ac9d-cdf38a119272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of files and stack them as a 3D numpy array\n",
    "def load_group(filenames, prefix=''):\n",
    "    \"\"\"\n",
    "    Load multiple files and stack them into a single 3D numpy array.\n",
    "    \n",
    "    Parameters:\n",
    "    filenames (list): List of filenames to load.\n",
    "    prefix (str): Prefix path to the files.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The stacked data.\n",
    "    \"\"\"\n",
    "    loaded = [load_file(prefix + name) for name in filenames]\n",
    "    return np.dstack(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32419c9f-2bfb-49da-a47d-9b8caec2a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the total accelerometer data (x, y, z) for training\n",
    "filenames = ['total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
    "total_acc = load_group(filenames, prefix='HARDataset/train/Inertial Signals/')\n",
    "print(total_acc.shape)\n",
    "# (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3edf3513-518a-4e91-845b-bf4787af7d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.012817  , -0.1232167 ,  0.1029341 ],\n",
       "       [ 1.022833  , -0.1268756 ,  0.1056872 ],\n",
       "       [ 1.022028  , -0.1240037 ,  0.1021025 ],\n",
       "       [ 1.017877  , -0.1249279 ,  0.1065527 ],\n",
       "       [ 1.02368   , -0.1257667 ,  0.1028135 ],\n",
       "       [ 1.016974  , -0.124462  ,  0.1074931 ],\n",
       "       [ 1.017746  , -0.1273606 ,  0.1093857 ],\n",
       "       [ 1.019263  , -0.1278912 ,  0.1038862 ],\n",
       "       [ 1.016417  , -0.1258682 ,  0.1024732 ],\n",
       "       [ 1.020745  , -0.1243682 ,  0.0975659 ],\n",
       "       [ 1.018643  , -0.1231382 ,  0.09764665],\n",
       "       [ 1.019521  , -0.1213345 ,  0.09537356],\n",
       "       [ 1.02026   , -0.1183578 ,  0.09367106],\n",
       "       [ 1.018041  , -0.120062  ,  0.09921876],\n",
       "       [ 1.020829  , -0.1221186 ,  0.09997368],\n",
       "       [ 1.018644  , -0.12008   ,  0.09889572],\n",
       "       [ 1.019398  , -0.1209017 ,  0.0962825 ],\n",
       "       [ 1.020399  , -0.1213949 ,  0.09765831],\n",
       "       [ 1.019222  , -0.1215677 ,  0.1004408 ],\n",
       "       [ 1.022093  , -0.1246812 ,  0.09846986],\n",
       "       [ 1.020433  , -0.1254896 ,  0.1015699 ],\n",
       "       [ 1.020534  , -0.1249345 ,  0.1021059 ],\n",
       "       [ 1.021503  , -0.1249063 ,  0.1005147 ],\n",
       "       [ 1.019931  , -0.1249926 ,  0.1021144 ],\n",
       "       [ 1.02048   , -0.1251552 ,  0.100956  ],\n",
       "       [ 1.018945  , -0.1247985 ,  0.1014768 ],\n",
       "       [ 1.019238  , -0.1254793 ,  0.1014304 ],\n",
       "       [ 1.019989  , -0.1268068 ,  0.1000858 ],\n",
       "       [ 1.018917  , -0.1272888 ,  0.09980876],\n",
       "       [ 1.019762  , -0.123713  ,  0.09743248],\n",
       "       [ 1.019021  , -0.1192631 ,  0.09520512],\n",
       "       [ 1.017887  , -0.1226967 ,  0.09369074],\n",
       "       [ 1.018136  , -0.1271224 ,  0.09435198],\n",
       "       [ 1.019543  , -0.126278  ,  0.09595278],\n",
       "       [ 1.020242  , -0.1261419 ,  0.0986602 ],\n",
       "       [ 1.018757  , -0.1251686 ,  0.1049836 ],\n",
       "       [ 1.019534  , -0.121594  ,  0.1066249 ],\n",
       "       [ 1.019862  , -0.1190558 ,  0.1064143 ],\n",
       "       [ 1.01906   , -0.1179128 ,  0.109485  ],\n",
       "       [ 1.020717  , -0.1174034 ,  0.107602  ],\n",
       "       [ 1.021055  , -0.1172102 ,  0.1030893 ],\n",
       "       [ 1.020178  , -0.1181487 ,  0.1005282 ],\n",
       "       [ 1.018108  , -0.1185709 ,  0.09970745],\n",
       "       [ 1.014776  , -0.1179084 ,  0.1006193 ],\n",
       "       [ 1.015374  , -0.1205067 ,  0.1000773 ],\n",
       "       [ 1.018429  , -0.1243031 ,  0.1021935 ],\n",
       "       [ 1.019895  , -0.1256299 ,  0.107519  ],\n",
       "       [ 1.018647  , -0.1246896 ,  0.108948  ],\n",
       "       [ 1.016387  , -0.1218014 ,  0.107225  ],\n",
       "       [ 1.017053  , -0.1202801 ,  0.1045164 ],\n",
       "       [ 1.019572  , -0.1206562 ,  0.102301  ],\n",
       "       [ 1.021097  , -0.1210648 ,  0.09824694],\n",
       "       [ 1.019488  , -0.1216185 ,  0.09383632],\n",
       "       [ 1.017218  , -0.1241114 ,  0.1008436 ],\n",
       "       [ 1.019876  , -0.1280997 ,  0.1089435 ],\n",
       "       [ 1.022022  , -0.1280257 ,  0.1037146 ],\n",
       "       [ 1.020574  , -0.126537  ,  0.1004966 ],\n",
       "       [ 1.021588  , -0.1274474 ,  0.1040357 ],\n",
       "       [ 1.022298  , -0.1273523 ,  0.1024968 ],\n",
       "       [ 1.019369  , -0.1264597 ,  0.1001817 ],\n",
       "       [ 1.01698   , -0.1247455 ,  0.1002736 ],\n",
       "       [ 1.016774  , -0.1236691 ,  0.09922501],\n",
       "       [ 1.016079  , -0.1229069 ,  0.09939194],\n",
       "       [ 1.015292  , -0.1215528 ,  0.0985878 ],\n",
       "       [ 1.018851  , -0.123976  ,  0.09792958],\n",
       "       [ 1.02238   , -0.1268078 ,  0.09935086],\n",
       "       [ 1.020781  , -0.1277862 ,  0.09811381],\n",
       "       [ 1.020218  , -0.1266547 ,  0.09751712],\n",
       "       [ 1.021344  , -0.1236336 ,  0.0974719 ],\n",
       "       [ 1.020522  , -0.1249187 ,  0.09657678],\n",
       "       [ 1.01979   , -0.1243005 ,  0.09774064],\n",
       "       [ 1.019216  , -0.1197982 ,  0.09730921],\n",
       "       [ 1.018307  , -0.1192223 ,  0.09815882],\n",
       "       [ 1.017996  , -0.120174  ,  0.1011657 ],\n",
       "       [ 1.017932  , -0.1213162 ,  0.1016211 ],\n",
       "       [ 1.018121  , -0.1222915 ,  0.1015734 ],\n",
       "       [ 1.018305  , -0.1264941 ,  0.1011669 ],\n",
       "       [ 1.018458  , -0.132631  ,  0.1013595 ],\n",
       "       [ 1.018201  , -0.1286242 ,  0.1011384 ],\n",
       "       [ 1.017129  , -0.122015  ,  0.09923839],\n",
       "       [ 1.017814  , -0.1218544 ,  0.1005806 ],\n",
       "       [ 1.0188    , -0.1212108 ,  0.1025034 ],\n",
       "       [ 1.017601  , -0.120603  ,  0.1001684 ],\n",
       "       [ 1.01797   , -0.1209479 ,  0.09859667],\n",
       "       [ 1.018489  , -0.1212181 ,  0.1007798 ],\n",
       "       [ 1.017787  , -0.1236811 ,  0.1003598 ],\n",
       "       [ 1.019167  , -0.1255531 ,  0.09920002],\n",
       "       [ 1.019789  , -0.1241987 ,  0.1017142 ],\n",
       "       [ 1.019462  , -0.1221575 ,  0.101472  ],\n",
       "       [ 1.020433  , -0.1233974 ,  0.1012259 ],\n",
       "       [ 1.021189  , -0.127263  ,  0.1026532 ],\n",
       "       [ 1.021903  , -0.1302381 ,  0.1016078 ],\n",
       "       [ 1.021936  , -0.1312862 ,  0.1006335 ],\n",
       "       [ 1.02055   , -0.1299998 ,  0.09614245],\n",
       "       [ 1.018878  , -0.1277708 ,  0.09032558],\n",
       "       [ 1.018548  , -0.1239092 ,  0.0899055 ],\n",
       "       [ 1.017389  , -0.1197379 ,  0.0931167 ],\n",
       "       [ 1.015021  , -0.121468  ,  0.09574084],\n",
       "       [ 1.01931   , -0.1260118 ,  0.09424991],\n",
       "       [ 1.024606  , -0.1282198 ,  0.09448054],\n",
       "       [ 1.021863  , -0.1263671 ,  0.09775536],\n",
       "       [ 1.020201  , -0.1217475 ,  0.09900205],\n",
       "       [ 1.020573  , -0.1218799 ,  0.09852306],\n",
       "       [ 1.018729  , -0.1239658 ,  0.09542946],\n",
       "       [ 1.01936   , -0.1242567 ,  0.0942117 ],\n",
       "       [ 1.019954  , -0.1261499 ,  0.09773522],\n",
       "       [ 1.018969  , -0.1276499 ,  0.1021875 ],\n",
       "       [ 1.019633  , -0.1295878 ,  0.1034573 ],\n",
       "       [ 1.019553  , -0.1303476 ,  0.09961019],\n",
       "       [ 1.019179  , -0.1299795 ,  0.09758879],\n",
       "       [ 1.019695  , -0.1321647 ,  0.09861568],\n",
       "       [ 1.019145  , -0.12887   ,  0.09788232],\n",
       "       [ 1.018516  , -0.124246  ,  0.09687653],\n",
       "       [ 1.017926  , -0.1267462 ,  0.09700962],\n",
       "       [ 1.01778   , -0.1270445 ,  0.09758221],\n",
       "       [ 1.018917  , -0.1256371 ,  0.09707332],\n",
       "       [ 1.020606  , -0.12678   ,  0.09742485],\n",
       "       [ 1.022583  , -0.1258547 ,  0.0993411 ],\n",
       "       [ 1.020981  , -0.1234932 ,  0.1000578 ],\n",
       "       [ 1.018065  , -0.1219945 ,  0.09856404],\n",
       "       [ 1.019638  , -0.1239103 ,  0.09317716],\n",
       "       [ 1.020017  , -0.1279703 ,  0.08874205],\n",
       "       [ 1.018766  , -0.1282954 ,  0.09050464],\n",
       "       [ 1.019815  , -0.1270103 ,  0.09484291],\n",
       "       [ 1.01929   , -0.1261848 ,  0.09835045],\n",
       "       [ 1.018445  , -0.1240696 ,  0.1003852 ],\n",
       "       [ 1.019372  , -0.1227451 ,  0.09987355],\n",
       "       [ 1.021171  , -0.121326  ,  0.09498741]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_acc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f93fa-907b-4f31-80e4-b964123ffc9a",
   "metadata": {},
   "source": [
    "- Given the parallel structure of the train and test folders, we will create a new function to load input and output data for a specified folder.\n",
    "- This function compiles a list of the 9 data files, combines them into a NumPy array with 9 features, and loads the output class data.\n",
    "- The load_dataset() function below can be used for either the train or test group by passing the group name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06c437f6-83ce-4825-8bbe-1c0a018fb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset group, such as train or test\n",
    "def load_dataset(group, prefix=''):\n",
    "    \"\"\"\n",
    "    Load all data for a given dataset group (train or test).\n",
    "    \n",
    "    Parameters:\n",
    "    group (str): The dataset group to load ('train' or 'test').\n",
    "    prefix (str): Prefix path to the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Input data (X) and output class data (y).\n",
    "    \"\"\"\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # List all 9 data files to load\n",
    "    filenames = [\n",
    "        # Total acceleration\n",
    "        'total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt',\n",
    "        # Body acceleration\n",
    "        'body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt',\n",
    "        # Body gyroscope\n",
    "        'body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt'\n",
    "    ]\n",
    "    # Load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # Load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c31b3210-131d-4196-b1d6-159ef1c569d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X_train, y_train = load_dataset('train', prefix='HARDataset/')\n",
    "X_test, y_test = load_dataset('test', prefix='HARDataset/')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
